models:
  sd15:
    name: "Stable Diffusion 1.5"
    description: "Classic diffusion model, great for general use"
    supports_negative_prompt: true
    default_resolution: 512
    min_resolution: 256
    max_resolution: 768
    backends:
      pytorch:
        base_model: "runwayml/stable-diffusion-v1-5"
        quantizations:
          fp16:
            memory: "~4GB VRAM"
            description: "Full precision, best quality"
            available: true
      onnx:
        base_model: "runwayml/stable-diffusion-v1-5"
        quantizations:
          fp32:
            memory: "~4GB"
            description: "ONNX FP32, CPU/GPU compatible, controlled repo"
            available: true
            model_path: "Mitchins/sd15-onnx-cpu"

  sdxl:
    name: "Stable Diffusion XL"
    description: "High-quality large diffusion model"
    supports_negative_prompt: true
    default_resolution: 1024
    min_resolution: 512
    max_resolution: 1536
    backends:
      pytorch:
        base_model: "stabilityai/stable-diffusion-xl-base-1.0"
        quantizations:
          fp16:
            memory: "~8GB VRAM"
            description: "Full quality, GPU required"
            available: true
      onnx:
        base_model: "stabilityai/stable-diffusion-xl-base-1.0"
        quantizations:
          fp32:
            memory: "~12GB"
            description: "ONNX FP32, CPU/GPU compatible, controlled repo"
            available: true
            model_path: "Mitchins/sdxl-onnx-cpu"

  sdxl-turbo:
    name: "Stable Diffusion XL Turbo"
    description: "Fast SDXL variant, fewer steps needed"
    supports_negative_prompt: true
    default_resolution: 1024
    min_resolution: 512
    max_resolution: 1024
    backends:
      pytorch:
        base_model: "stabilityai/sdxl-turbo"
        quantizations:
          fp16:
            memory: "~8GB VRAM"
            description: "Fast generation, fewer steps"
            available: true
      onnx:
        base_model: "stabilityai/sdxl-turbo"
        quantizations:
          fp32:
            memory: "~12GB"
            description: "ONNX FP32, CPU/GPU compatible"
            available: false
            note: "Creation failed due to disk space constraints"

  flux1:
    name: "FLUX.1 Schnell"
    description: "Excellent for complex artistic prompts"
    supports_negative_prompt: false
    default_resolution: 1024
    min_resolution: 256
    max_resolution: 1024
    backends:
      pytorch:
        base_model: "black-forest-labs/FLUX.1-schnell"
        quantizations:
          fp16:
            memory: "~12GB VRAM"
            description: "Best quality for complex artistic prompts"
            available: true

# Runtime mapping - how legacy model IDs map to config
model_id_mapping:
  # Working models only
  "sd15": ["sd15", "pytorch", "fp16"]
  "sd15-onnx": ["sd15", "onnx", "fp32"]
  "sdxl": ["sdxl", "pytorch", "fp16"] 
  "sdxl-onnx": ["sdxl", "onnx", "fp32"]
  "sdxl-turbo": ["sdxl-turbo", "pytorch", "fp16"]
  "sdxl-turbo-onnx": ["sdxl-turbo", "onnx", "fp32"]
  "flux1-schnell": ["flux1", "pytorch", "fp16"]

# UI display configuration
ui_config:
  backend_display:
    pytorch: "PyTorch (GPU)"
    onnx: "ONNX (CPU/GPU)"
  
  quantization_display:
    fp16: "FP16"
    q8: "Q8 (8-bit)"
    q4: "Q4 (4-bit)"

  default_selections:
    # Prefer models with good memory/quality balance
    preferred_models:
      - ["sd15", "onnx", "fp16"]      # Best CPU/GPU balance (2GB memory)
      - ["flux1", "pytorch", "q8"]    # Good GPU balance
      - ["sdxl", "pytorch", "fp16"]   # High-end GPU