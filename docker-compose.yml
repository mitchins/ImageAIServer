version: '3.8'

services:
  # Base service - ONNX only (lightweight ~2GB)
  imageai-base:
    build:
      context: .
      dockerfile: Dockerfile.base
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - HF_HOME=/app/.cache/huggingface
      - BACKEND_TYPE=onnx
      - LOG_LEVEL=info
    volumes:
      - model_cache:/app/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    profiles: ["base", "all"]

  # Full service - ONNX + PyTorch (~8GB)
  imageai-torch:
    build:
      context: .
      dockerfile: Dockerfile.torch
      args:
        TORCH_VERSION: cpu
    ports:
      - "8001:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - HF_HOME=/app/.cache/huggingface
      - BACKEND_TYPE=auto
      - LOG_LEVEL=info
    volumes:
      - model_cache:/app/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["torch", "all"]

  # GPU-enabled PyTorch service (requires nvidia-docker)
  imageai-torch-gpu:
    build:
      context: .
      dockerfile: Dockerfile.torch
      args:
        TORCH_VERSION: cu121
    ports:
      - "8002:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - HF_HOME=/app/.cache/huggingface
      - BACKEND_TYPE=auto
      - LOG_LEVEL=info
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - model_cache:/app/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["gpu"]

  # Legacy service (uses original Dockerfile for backwards compatibility)
  imageaiserver:
    build: .
    ports:
      - "8003:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - HF_HOME=/app/.cache/huggingface
    volumes:
      - model_cache:/app/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["legacy"]

volumes:
  model_cache:
    driver: local